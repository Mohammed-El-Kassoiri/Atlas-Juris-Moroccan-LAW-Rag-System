# streamlit_app.py
# Streamlit UI for Local RAG System using local embeddings + FAISS and Gemini (Google) generation
# Requirements:
#   pip install streamlit sentence-transformers faiss-cpu google-genai
# Run:
#   streamlit run streamlit_app.py

import streamlit as st
import os, json, time, numpy as np, faiss, pickle, traceback, re, textwrap
from pathlib import Path
from sentence_transformers import SentenceTransformer

# Gemini SDK
try:
    from google import genai
except Exception:
    genai = None

# ===== CONFIG (edit these paths if needed) =====
FLATTENED_JSONL = r"data/processed/flattened_docs.jsonl"
INDEX_FILE = r"law_index.faiss"
META_FILE = r"law_meta.pkl"
EMB_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"  # embedding model

TOP_K = 3
MAX_NEW_TOKENS = 512
DEVICE = "cpu"
# ===============================================

st.set_page_config(page_title="Ù…Ø­Ø§Ù…ÙŠ Ø§ÙØªØ±Ø§Ø¶ÙŠ - Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ØºØ±Ø¨", page_icon="âš–ï¸", layout="wide")
st.title("âš–ï¸ Ù…Ø­Ø§Ù…ÙŠ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø­Ù„ÙŠ â€” Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ØºØ±Ø¨")
st.caption("ğŸ”§ ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠÙ‹Ø§ Ù„Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ØŒ ÙˆÙŠØ³ØªØ®Ø¯Ù… Gemini Ù„Ù„ØªÙˆÙ„ÙŠØ¯ (Ø¶Ø¹ GEMINI_API_KEY ÙƒÙ…ØªØºÙŠØ± Ø¨ÙŠØ¦ÙŠ)")

# ----------------- helpers for normalization (for metadata matching) -----------------
ARABIC_DIACRITICS_RE = re.compile(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06DC\u06DF-\u06E8\u06EA-\u06ED]')
ARABIC_STOPWORDS = {"Ù…Ù†","ÙÙŠ","Ø¹Ù„Ù‰","Ø¥Ù„Ù‰","Ø¹Ù†","Ù…Ø§","Ù‡Ùˆ","Ù‡ÙŠ","Ù„Ù…","Ù„Ù†","Ø¥Ù†","Ø£Ù†","ÙƒÙ„","Ù‚Ø¯","Ø£Ùˆ","Ùˆ","Ø§Ù„ØªÙŠ","Ø§Ù„Ø°ÙŠ","Ø§Ù„Ø°ÙŠÙ†","Ù‡Ø°Ø§","Ù‡Ø°Ù‡","Ø°Ù„Ùƒ","ØªÙ„Ùƒ","Ù…Ø¹","Ø£Ù†Ù‘","Ø¥Ù„Ø§","ÙƒØ§Ù†","ÙƒØ§Ù†Øª","Ù‡Ù†Ø§Ùƒ","Ø£ÙŠ","Ø³ÙˆØ§Ø¡","Ø¨Ø¹Ø¯","Ù‚Ø¨Ù„","Ø­ØªÙ‰"}

def strip_diacritics_arabic(text: str) -> str:
    return ARABIC_DIACRITICS_RE.sub("", text) if text else ""

def normalize_text_for_match(s: str) -> str:
    if not s:
        return ""
    s = strip_diacritics_arabic(s)
    s = s.lower()
    s = re.sub(r"[^\w\u0600-\u06FF]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def strip_prefixes(token: str) -> str:
    if not token:
        return token
    t = token
    while t.startswith("Ùˆ") and len(t) > 1:
        t = t[1:]
    if t.startswith("Ø§Ù„") and len(t) > 2:
        t = t[2:]
    if t.startswith("Ùˆ") and len(t) > 1:
        t = t[1:]
    return t

def tokenize_and_clean(s: str):
    if not s:
        return []
    s_norm = normalize_text_for_match(s)
    tokens = [t for t in s_norm.split() if t]
    processed = []
    for t in tokens:
        t2 = strip_prefixes(t).strip()
        if not t2:
            continue
        if t2 in ARABIC_STOPWORDS:
            continue
        processed.append(t2)
    return processed

def token_overlap_score(query_norm: str, meta_norm: str) -> float:
    if not query_norm or not meta_norm:
        return 0.0
    q_toks = set(tokenize_and_clean(query_norm))
    m_toks = set(tokenize_and_clean(meta_norm))
    if not q_toks or not m_toks:
        return 0.0
    shared = q_toks.intersection(m_toks)
    return len(shared) / max(1, len(q_toks))

# ---------- Simple language detection (heuristic) ----------
ARABIC_CHAR_RE = re.compile(r'[\u0600-\u06FF]')
LATIN_CHAR_RE = re.compile(r'[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿]')

def detect_language(s: str) -> str:
    """Very simple heuristic: returns 'ar' for Arabic-heavy text, 'fr' for Latin-heavy,
    otherwise 'other'."""
    if not s or not isinstance(s, str):
        return "other"
    ar_count = len(ARABIC_CHAR_RE.findall(s))
    lat_count = len(LATIN_CHAR_RE.findall(s))
    # bias threshold: prefer Arabic if Arabic chars >= Latin chars
    if ar_count > 0 and ar_count >= lat_count:
        return "ar"
    if lat_count > 0 and lat_count > ar_count:
        return "fr"
    return "other"

# --- prepare index & metas (build if missing) ---
def prepare_index_and_meta():
    flat = Path(FLATTENED_JSONL)
    if not flat.exists():
        raise FileNotFoundError(f"{FLATTENED_JSONL} not found. Put your flattened_docs.jsonl at that path.")

    texts = []
    metas = []
    with flat.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            text = obj.get("text","")
            # ensure metadata fields exist
            mada = obj.get("mada") or obj.get("id", f"m{i+1}")
            bab  = obj.get("bab") or obj.get("fasl") or ""
            source = obj.get("source") or bab or ""
            _id = obj.get("id") or f"{i+1:05d}"
            texts.append(text)
            # detect language of this snippet (fallback to source/title too)
            lang = detect_language(text or mada or bab or source)
            metas.append({"id": _id, "mada": mada, "bab": bab, "source": source, "lang": lang})

    # if index and meta pickle exist, load them to speed up
    if Path(INDEX_FILE).exists() and Path(META_FILE).exists():
        try:
            # validate meta file; ensure 'lang' exists for each meta (backwards compatibility)
            index = faiss.read_index(INDEX_FILE)
            with open(META_FILE, "rb") as f:
                meta_pkl = pickle.load(f)
            if isinstance(meta_pkl, list) and len(meta_pkl) == len(texts):
                # ensure each meta has 'lang'
                for i, m in enumerate(meta_pkl):
                    if "lang" not in m or not m.get("lang"):
                        meta_pkl[i]["lang"] = detect_language(texts[i] or m.get("mada") or m.get("bab") or m.get("source") or "")
                metas = meta_pkl
        except Exception:
            # fall through to rebuild embeddings if anything goes wrong
            pass
    else:
        st.info("Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©) â€” Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª...")
        embedder = SentenceTransformer(EMB_MODEL_NAME)
        embs = embedder.encode(texts, show_progress_bar=True, convert_to_numpy=True)
        if embs.dtype != np.float32:
            embs = embs.astype(np.float32)
        faiss.normalize_L2(embs)
        dim = embs.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(embs)
        faiss.write_index(index, INDEX_FILE)
        with open(META_FILE, "wb") as f:
            pickle.dump(metas, f)
        st.success("ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆØ­ÙØ¸Ù‡.")
    return texts, metas

# --- load index & embed model (cached) ---
@st.cache_resource
def load_index_and_embedder():
    texts, metas = prepare_index_and_meta()
    index = faiss.read_index(INDEX_FILE)
    embed_model = SentenceTransformer(EMB_MODEL_NAME)
    return index, texts, metas, embed_model

with st.spinner("â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆÙ†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†..."):
    index, texts, metas, embed_model = load_index_and_embedder()
st.success(f"âœ… Ø§Ù„ÙÙ‡Ø±Ø³ Ø¬Ø§Ù‡Ø² â€” Ù…Ù‚ØªØ·ÙØ§Øª: {len(texts)}")

# --- retrieval helpers ---
def embed_query(text):
    v = embed_model.encode([text], convert_to_numpy=True)
    if v.dtype != np.float32:
        v = v.astype(np.float32)
    faiss.normalize_L2(v)
    return v

def retrieve(query, top_k=TOP_K, prefer_same_language=True, strict_same_language=False):
    """
    Retrieve top_k candidates, optionally preferring or strictly filtering to same-language snippets.
    If strict_same_language is True, only returns docs where meta.lang == detected query language
    (may return fewer than top_k if not enough matches).
    """
    q_lang = detect_language(query)
    qv = embed_query(query)
    # retrieve a larger candidate pool to allow filtering/re-ranking
    D, I = index.search(qv, max(top_k * 6, top_k))
    candidates = []
    for score, idx in zip(D[0], I[0]):
        if idx < 0 or idx >= len(texts):
            continue
        candidates.append({"score": float(score), "idx": int(idx), "text": texts[idx], "meta": metas[idx]})

    if strict_same_language and q_lang in ("ar", "fr"):
        same_lang = [c for c in candidates if c.get("meta", {}).get("lang") == q_lang]
        return sorted(same_lang, key=lambda x: x["score"], reverse=True)[:top_k]

    if prefer_same_language and q_lang in ("ar", "fr"):
        same_lang = [c for c in candidates if c.get("meta", {}).get("lang") == q_lang]
        if len(same_lang) >= top_k:
            selected = sorted(same_lang, key=lambda x: x["score"], reverse=True)[:top_k]
            return selected
        # otherwise take as many same-lang as possible then fill with best others
        selected = sorted(same_lang, key=lambda x: x["score"], reverse=True)
        others = [c for c in candidates if c.get("meta", {}).get("lang") != q_lang]
        others_sorted = sorted(others, key=lambda x: x["score"], reverse=True)
        selected.extend(others_sorted[: max(0, top_k - len(selected))])
        return selected

    # default: just top_k by score
    out = sorted(candidates, key=lambda x: x["score"], reverse=True)[:top_k]
    return out

# ---------- Gemini helpers ----------
def build_instructional_prompt_from_retrieved(query, retrieved):
    """Create the single combined prompt instructing Gemini to produce the structured legal answer.
       The prompt language matches the detected query language (ar/fr)."""
    # Build a context with numbered sources
    context_parts = []
    for i, r in enumerate(retrieved, start=1):
        m = r.get("meta", {})
        mada = m.get("mada", "")
        bab = m.get("bab", "")
        src = m.get("source", "") or bab or ""
        text = r.get("text", "")
        snippet = text if len(text) <= 4000 else text[:4000] + " ...[truncated]"
        # localize source label depending on expected language -- default Arabic label used below, replaced for French if needed
        context_parts.append(f"Ø§Ù„Ù…ØµØ¯Ø± {i}: ({mada} : {bab} : {src})\n{snippet}")

    context = "\n\n".join(context_parts)

    q_lang = detect_language(query)
    if q_lang == "fr":
        # replace Arabic "Ø§Ù„Ù…ØµØ¯Ø±" labels with French ones in the context
        context = context.replace("Ø§Ù„Ù…ØµØ¯Ø± ", "Source ")
        system_line = "SYSTEM: Vous Ãªtes un avocat virtuel spÃ©cialisÃ© en droit marocain."
        instr = textwrap.dedent(f"""\ 
        Reformulez les extraits suivants et produisez une rÃ©ponse juridique unique et structurÃ©e â€” en franÃ§ais â€” comprenant, dans l'ordre :
        1) Un rÃ©sumÃ© bref (2-3 phrases).
        2) Une analyse juridique dÃ©taillÃ©e en s'appuyant exclusivement sur les extraits, en citant aprÃ¨s chaque point (Article : Chapitre : Source).
        3) Une conclusion / conseil pratique court.
        4) Liste des rÃ©fÃ©rences utilisÃ©es.

        Ne rajoutez pas d'informations extÃ©rieures aux extraits. Si les extraits sont insuffisants, indiquez-le clairement.

        Extraits:
        --------------------
        {context}
        --------------------

        Exigence : rÃ©ponse organisÃ©e avec sous-titres (RÃ©sumÃ©, Analyse juridique, Conclusion/Conseil, RÃ©fÃ©rences).
        """)
    else:
        # default to Arabic
        system_line = "SYSTEM: Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØºØ±Ø¨ÙŠ."
        instr = textwrap.dedent(f"""\ 
        Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ£Ù†ØªØ¬ Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ø­Ø¯Ø© ÙˆÙ…ØªÙƒØ§Ù…Ù„Ø© â€” Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ â€” ÙˆØªØªØ¶Ù…Ù† Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨:
        1) Ø®Ù„Ø§ØµØ© Ù…ÙˆØ¬Ø²Ø© (2-3 Ø¬Ù…Ù„).
        2) ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ÙØµÙ‘Ù„ ÙŠØ³ØªÙ†Ø¯ Ø­ØµØ±ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ù…Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¨Ø¹Ø¯ ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¨Ø§Ù„Ø´ÙƒÙ„ (Ø§Ù„Ù…Ø§Ø¯Ø© : Ø§Ù„Ø¨Ø§Ø¨ : Ø§Ù„Ù…ØµØ¯Ø±).
        3) Ø§Ø³ØªÙ†ØªØ§Ø¬ / Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ© Ù‚ØµÙŠØ±Ø©.
        4) Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©.

        Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª ÙˆÙ„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø®Ø§Ø±Ø¬Ù‡Ø§. Ø¥Ù† ÙƒØ§Ù†Øª Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© ÙØ§Ø°ÙƒØ± Ø°Ù„Ùƒ ØµØ±Ø§Ø­Ø©.

        Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª:
        --------------------
        {context}
        --------------------

        Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù†Ø¸Ù…Ø© Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ©: (Ø§Ù„Ø®Ù„Ø§ØµØ©ØŒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØŒ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬/Ø§Ù„Ù†ØµÙŠØ­Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŒ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹).
        """)
    full_prompt = f"{system_line}\nQUESTION: {query}\n\n{instr}\nANSWER:\n"
    return full_prompt

def extract_text_from_gemini_response(resp) -> str:
    """Robust extractor for various SDK response shapes."""
    try:
        if hasattr(resp, "text"):
            txt = resp.text
            if callable(txt):
                txt = txt()
            if txt:
                return txt
    except Exception:
        pass

    try:
        candidates = getattr(resp, "candidates", None) or getattr(resp, "Candidates", None)
        if candidates:
            first = candidates[0]
            # many shapes: try common traversals
            for attr in ("content", "Content"):
                cont = getattr(first, attr, None) or (first.get(attr) if isinstance(first, dict) else None)
                if cont:
                    parts = getattr(cont, "parts", None) or (cont.get("parts") if isinstance(cont, dict) else None)
                    if parts and len(parts) > 0:
                        p0 = parts[0]
                        if isinstance(p0, dict):
                            t = p0.get("text") or p0.get("Text")
                        else:
                            t = getattr(p0, "text", None) or getattr(p0, "Text", None)
                        if t:
                            return t
            if hasattr(first, "text"):
                t = first.text
                if callable(t):
                    t = t()
                if t:
                    return t
    except Exception:
        pass

    # fallback: return stringified object for debugging
    try:
        return str(resp)
    except Exception:
        return None

def call_gemini_generate(prompt, model_name="gemini-2.5-flash", max_output_tokens=None, temperature=0.0):
    """Call Gemini (google-genai). Returns generated text or raises error."""
    api_key = "AIzaSyArOg9PSDtMQAOLAXERLdShaaSSxEnj_J8"  # expect user to set this in environment
    if genai is None:
        raise RuntimeError("google-genai library not installed. pip install google-genai")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY not found in environment. Set it before running.")

    client = genai.Client(api_key=api_key)
    kwargs = {"model": model_name, "contents": prompt}
    try:
        resp = client.models.generate_content(**kwargs)
    except Exception as e:
        raise

    text = extract_text_from_gemini_response(resp)
    return text

# --- Streamlit UI state & controls ---
if "history" not in st.session_state:
    st.session_state.history = []

with st.sidebar:
    st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    top_k = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©", 1, 8, TOP_K)
    max_tokens = st.slider("Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø­Ø¯ Ø£Ù‚ØµÙ‰) ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§", 128, 2048, MAX_NEW_TOKENS, step=64)
    temp = st.slider("Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (temperature)", 0.0, 1.0, 0.0, step=0.05)
    model_choice = st.selectbox("Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Gemini)", ["gemini-2.5-flash"], index=0)
    show_prompt = st.checkbox("Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø·Ù„Ø¨ (prompt) Ø§Ù„Ù…Ø±Ø³Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (Debug)", value=False)
    st.divider()
    st.subheader("Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù„ØºØ©")
    prefer_same_lang = st.checkbox("ÙØ¶Ù‘Ù„ Ù…Ù‚ØªØ·ÙØ§Øª Ø¨Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ (Ø§ÙØªØ±Ø§Ø¶ÙŠ)", value=True)
    strict_same_lang = st.checkbox("Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØµØ§Ø±Ù…Ø©: Ø¥Ø±Ø¬Ø§Ø¹ Ù…Ù‚ØªØ·ÙØ§Øª Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù„ØºØ© ÙÙ‚Ø· (Ù‚Ø¯ ÙŠØ¹ÙŠØ¯ Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)", value=False)
    st.divider()
    st.subheader("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
    st.metric("Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©", f"{len(texts):,}")
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ø³Ø¬Ù„"):
        st.session_state.history = []
        st.experimental_rerun()

# Main input
col1, col2 = st.columns([4,1])
with col1:
    query = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø¨Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©:", placeholder="Ù…Ø«Ø§Ù„ (Ø¹Ø±Ø¨ÙŠØ©): Ù…Ø§ Ù‡ÙŠ Ù…Ù‡Ø§Ù… Ù…Ø¤Ø³Ø³Ø© ...ØŸ â€” Ù…Ø«Ø§Ù„ (franÃ§ais): Quelles sont les rÃ¨gles du cautionnement de comparution ?")
with col2:
    ask_button = st.button("ğŸ” Ø§Ø³Ø£Ù„", use_container_width=True)

if ask_button and query:
    with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ§Ù„ØªÙˆÙ„ÙŠØ¯..."):
        t0 = time.time()
        retrieved = retrieve(query, top_k=top_k, prefer_same_language=prefer_same_lang, strict_same_language=strict_same_lang)
        retrieval_time = time.time() - t0

        if not retrieved:
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø°Ø§Øª ØµÙ„Ø©.")
        else:
            prompt = build_instructional_prompt_from_retrieved(query, retrieved)
            if show_prompt:
                st.subheader("ğŸ” Prompt (sent to Gemini)")
                st.code(prompt[:4000] + ("\n\n...[truncated]" if len(prompt) > 4000 else ""), language="text")

            gen_start = time.time()
            try:
                generated = call_gemini_generate(prompt, model_name=model_choice, temperature=float(temp))
            except Exception as e:
                st.error(f"ÙØ´Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹Ø¨Ø± Gemini: {e}")
                st.write(traceback.format_exc())
                generated = None
            generation_time = time.time() - gen_start
            total_time = time.time() - t0

            if generated:
                st.session_state.history.append({
                    "query": query,
                    "answer": generated,
                    "retrieved": retrieved,
                    "time": total_time,
                    "retrieval_time": retrieval_time,
                    "generation_time": generation_time
                })

# Display history
if st.session_state.history:
    st.divider()
    st.subheader("ğŸ“ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©")
    for i, item in enumerate(reversed(st.session_state.history), start=1):
        with st.container():
            st.markdown(f"### â“ {item['query']}")
            st.success(item["answer"])
            col_a, col_b, col_c = st.columns(3)
            with col_a:
                st.metric("â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ", f"{item['time']:.2f}s")
            with col_b:
                st.metric("ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹", f"{item['retrieval_time']:.2f}s")
            with col_c:
                st.metric("ğŸ¤– ØªÙˆÙ„ÙŠØ¯", f"{item['generation_time']:.2f}s")

            with st.expander("ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"):
                for j, r in enumerate(item["retrieved"], start=1):
                    m = r.get("meta", {})
                    mada = m.get("mada", "")
                    bab  = m.get("bab", "")
                    src  = m.get("source", "") or bab
                    lang = m.get("lang", "other")
                    st.markdown(f"- ({mada} : {bab} : {src}) â€” Ù„ØºØ©: {lang} â€” ØªØ´Ø§Ø¨Ù‡: {r['score']:.3f}")
                    st.markdown(f"> {r['text'][:600]}...")

            st.divider()
else:
    st.info("ğŸ‘† Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù„Ø¨Ø¯Ø¡")

st.divider()
st.caption("ğŸ¤– RAG Ù…Ø­Ù„ÙŠ + Gemini Ù„Ù„ØªÙˆÙ„ÙŠØ¯ | Ø§Ø¶Ø¨Ø· GEMINI_API_KEY ÙÙŠ Ø¨ÙŠØ¦ØªÙƒ Ù‚Ø¨Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„")